\documentclass{article}
\usepackage{helvet}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{arydshln}
\usepackage{xcolor}
\usepackage{titlesec}
\usepackage{microtype} % Prevents overfull hboxes by better text wrapping
\geometry{margin=1in}
\usepackage{subcaption}
\usepackage{listings}
\usepackage{color}
\newcommand{\bigCI}{\mathrel{\text{\scalebox{1.07}{$\perp\mkern-10mu\perp$}}}}
\usepackage{tikz}
\usetikzlibrary{arrows}
% Define colors for code syntax highlighting
\definecolor{codeblue}{rgb}{0.13, 0.13, 0.7}
\definecolor{codegreen}{rgb}{0, 0.5, 0}
\definecolor{codegray}{rgb}{0.5, 0.5, 0.5}
\definecolor{codepurple}{rgb}{0.58, 0, 0.82}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{white},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{codeblue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}

% Define colors
\definecolor{primary}{RGB}{0, 102, 204} % Blue color
\definecolor{IITBBlue}{RGB}{0, 51, 102} % IIT Bombay's signature blue

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{placeins}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{physics}




\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}


\title{Assignment 3 Report, CS 726: Spring 2024-25}
\author{
\IEEEauthorblockN{
    \begin{tabular}{cccc}
        \begin{minipage}[t]{0.23\textwidth}
            \centering
            Anupam Rawat\\
            IIT Bombay\\
            22b3982@iitb.ac.in \\
        \end{minipage}
        \begin{minipage}[t]{0.23\textwidth}
            \centering
            Aryan Gupta\\
            IIT Bombay\\
            22b2255@iitb.ac.in \\
        \end{minipage}
        \begin{minipage}[t]{0.23\textwidth}
            \centering
            Satush Parikh\\
            IIT Bombay\\
            21d070062@iitb.ac.in \\
        \end{minipage}
    \end{tabular}
}
}

\date{March 25, 2025}


\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{ulem,graphicx}
\usepackage[margin=0.5in]{geometry}

\begin{document}
\maketitle
\tableofcontents
\newpage
\\
\section{Introduction}
The assignment requires the implementation of several techniques for sequence generation, given a Large Language Model (in this case - \textbf{Llama 2-7b-hf}), evaluated on the English-to-Hindi translation task with \textbf{IN22-Gen dataset} using \textbf{BLEU Score} and \textbf{ROUGE Score}.

\subsection{Llama 2-7b-hf}
The recommended model is a pretrained Large Language Model with 7B parameters.The model has been developed by Meta, which was trained between January 2023 and July 2023. Llama 2 is an auto-regressive language model that uses optimized transformer architecture. The model is intended for use in English, thus the task of machine translation from Hindi to English in the IN22-Gen Dataset is a challenging task.

\subsection{Performance Metrics}
\begin{enumerate}
    \item \textbf{BLEU Scores (Bilingual Evaluation Understudy)} measures the precision of n-grams (unigrams, bigrams, trigrams, etc.) between the machine-generated translation and the reference translation. The metric focuses on precision and compares how many n-grams in the candidate translation match the reference translations.\\
    The score ranges between 0 to 1, higher score indicating a better translation quality. The metric tends to favor translations that use words that appear in the reference, but does not reward fluency, grammar, or meaning.

    \item \textbf{ROUGE Score (Recall-Oriented Understudy for Gisting Evaluation)} is a set of metrics primarily used for evaluating summarization and text generation, but can also be applied to machine translation. It measures recall by comparing the overlap of n-grams between the reference and the candidate output.\\
    Similar to BLEU, ROUGE score also ranges between 0 to 1, higher score indicating a better recall / better overlap.
    \begin{enumerate}
        \item \textit{ROUGE-1} measures unigram overlap (i.e. overlap of single words), between the reference and candidate translation.
        \item \textit{ROUGE-2} measures bigram overlap (i.e. overlap of a pair of consecutive words).
        \item \textit{ROUGE-LCS (Longest Common Sequence)} measures the longest common subsequence between the reference and candidate translation and focuses more on the sequence of words being in correct order, rather than plain overlaps. A higher ROUGE-LCS score suggests better preservation of the original meaning and structure in the translation.

    \end{enumerate}
\end{enumerate}

\subsection{IN22-Gen Dataset}
The IN22-Gen is a dataset consisting of 1024 sentences sourced from Wikepedia and other online sources spanning various domains ranging from culture and tourism to education and government. The dataset has the translation of these 1024 sentences from English to 22 Indic Languages(Hindi is one amongst the 22 languages). The task for this assignment is to convert the sentences from Hindi to English.

\section{Task 0: Introduction to LLM Decoding Techniques}
Contrary to the widespread assumption, the model doesn't output a single word or a sentence. Firstly, the input text is encoded with a Tokenizer, then this input\_token\_ids, are fed as an input to the model. The model outputs logits, which can be converted to probabilities using softmax function. The task of the decoding techniques is to select one particular token out of the tokens given with their probabilities. \newline
Below are the different widespread Decoding Techniques. These techniques are evaluated on two prominent metrics - \textbf{BLEU Score} and \textbf{ROUGE Score}, evaluated on the IN22 Dataset, for translation from Hindi to English tasks. For each of the Decoding techniques, the outputs are generated iteratively until the End-Of-Sequence (EOS) token is reached.

\subsection{Greedy Decoding}
For the case of Greedy Decoding, we pick the token with the highest probability among the 32,000 options (vocabulary size of Llama-2-7b-hf is 32k). Mathematically, at the $t^{th}$ step, the next token can be obtained:
\begin{equation}
    y_t = arg max_{w} P(w | y_{1:t-1}, x)
\end{equation}
The token selected at $t^{th}$ is the token with the maximum probability, selected among the tokens generated when the input and the previously generated token are provided as input. \\
The greedy decoding technique, is not very far sighted and looks at just the next token. Thus, the greedy decoding technique produces moderate scores on the standard tasks.

\subsection{Random Sampling with Temperature Scaling}
Unlike, the Greedy Decoding method, where the most probable token was chosen, in this decoding method, we randomly sample from the probability distribution while adjusting its sharpness using a temperature parameter $\tau$. The probabilities are modified as follows and a token is randomly sampled from $P^{'}$.
\begin{equation}
    P^{'} (w|y_{1:t-1}, x) = \frac{P(w|y_{1:t-1}, x)^{1/\tau}}{\sum_{w^{'}\in V}P(w^{'}|y_{1:t-1}, x)^{1/\tau}}
\end{equation}
At lower $\tau$ values, BLEU and ROUGE scores are relatively higher, indicating better coherence and fidelity since, the generated translations are less random, therefore favouring higher-probability words.\\
At medium $\tau$ values, a noticeable drop in BLEU and ROUGE can be observed due to the increasing randomness. As the $\tau$ continues to increase, we observe a significant decrease in all scores, especially ROUGE-2 and ROUGE-LCS, which measure phrase coherence, suggesting that the model is choosing words too randomly.\\
As the tau increases, we observe a trade-off between accuracy and diversity. A value of \textbf{$\tau = 0.6$} seems to be best choice, balancing fluency and diversity.

\subsection{Top-k Sampling}
For the top-k decoding sampling technique, we choose the next token, based on the top-k most probable tokens. Firstly, we take the top-k tokens having the highest probabilities : $V_k = {w_1, w_2, ..., w_k}$, where $P(w_i) \geq P(w_{i+1})$ for $i < k$. Then the probabilities are normalized and a token is chosen randomly from $P^{'}$, until the generation of an EOS token:
\begin{equation}
    P^{'}(w) = 
    \begin{cases}
        \frac{P(w)}{\sum_{w^{'} \in V_k} P(w^{'})}, & \text{if w $\in V_k$} \\
        0, & \text{otherwise}\\
    \end{cases}    
\end{equation}

Now, it may be expected that having a larger k means, that the chances of choosing a token from the sample having a lesser probability increases and therefore the next token, might not provide the appropriate translation. But at the same time, having a larger k, also increases the diversity of the tokens, allowing you to have more meaningful representations. The value of \textbf{k = 6}, finds the perfect match between diversity and meaningfulness, therefore giving the best scores.

\newpage
\subsection{Nucleus Sampling}
Unlike previous methods, which relied on a fixed number of tokens, in this case, we dynamically choose the smallest set of tokens whose cumulative probability exceeds a threshold p, and then normalize the probabilites:
\[
    V_p = \{w_1, w_2, ..., w_m\}\text{ such that }\sum_{i=1}^{m}P(w_i) \ge p
\]
\[
    P^{'}(w) = 
    \begin{cases}
    \frac{P(w)}{\sum_{w^{'}\in V_p} P(w^{'})} \text{ if w} \in V_p\\
    0 \text{ otherwise}
    \end{cases}
\]
Lower p-values correspond to higher BLEU \& ROUGE Scores, with \textbf{p = 0.5} achieving the best overall performance, indicating a balance between coherence and controlled diversity. Constraining the nucleus size ensures allows high-probability words to dominate while still allowing some variation.\\
As the p value increases to 0.7, the scores slightly decline, since more low-probability words are introduced and translations start diverging from reference texts. As the p value continues to increase, we see a sharp drop in the scores indicating that now along with diversity, a lot of randomness has also crept in, therefore impacting translation accuracy.\\
A p value of 0.5 is optimal as it ensures high-quality, fluent, and coherent translations. And a p value of 0.6-0.7 can be used when slightly more diverse translations are acceptable.

\subsection{Results and Conclusion}
The results highlight the trade-offs between determinism, diversity, and fluency in different decoding techniques for Hindi-to-English translation using LLaMA 2. \begin{itemize}
    \item Greedy decoding achieves the highest BLEU and ROUGE-1 but lacks diversity, often leading to repetitive outputs.
    \item Random sampling with temperature scaling ($\tau$ = 0.5) performs well but declines at higher temperatures due to excessive randomness.
    \item Top-k sampling (k = 6) achieves a balance but struggles with coherence at higher k-values.
    \item Nucleus sampling (p = 0.5) offers a strong trade-off, maintaining coherence while allowing some diversity.
\end{itemize} 
Overall, Greedy decoding is best for accuracy, while nucleus sampling (p = 0.5) provides the best balance between fluency and diversity.\\
These tasks were also run on the \textbf{Llama-3.1-8B} parameters, which produced a far better results. This is due to the fact that, this model is trained on a more diverse dataset, uses a better tokenization strategy, and has 1 Billion more parameters compared to Llama-2, therefore giving overall better results.\\

\section{Task 1: Word Constrained Decoding}
\subsection{Introduction}
Typically in sentences, once you have a starting portion, the range of possible next tokens decreases rapidly, given the sentence formation, structure, setting, and other factors. \textbf{Grammar Constrained Decoding} uses this property to get more accurate outputs from the LLM. In this exercise, we implement an alternate version, \textbf{Word-Constrained Decoding}, where we utilize a predefined set of words ("bag of words") to guide the output generation and therefore improve the performance of the LLM.

\subsection{Methodology}
Now, a word neccessarily can't be represented by a signle token, it might require a sequence of tokens for complete representation. Thus, to implement Word-Constrained Decoding, we employ a \textbf{Trie}-based approach to efficiently manage valid word sequences. The main steps of the algorithm are as follows:

\begin{enumerate}
    \item \textbf{Building the Trie}: The bag of words is tokenized and inserted into a trie, where each node represents a token and also contains a flag at to indicate the completion of the word.
    \item \textbf{Next Token} is chosen using the trie structure, to account for a valid word and via greedy decoding.
    \item \textbf{Valid Token Masking}: At each step, only tokens corresponding to valid next token are considered.
    \item \textbf{End-of-Word Handling}: Upon reaching the end of a word in the trie, the decoding process resets.
\end{enumerate}

\subsection{Implementation Details}
\subsubsection{TrieNode}
The Trie is implemented using \textit{TrieNode}. Each TrieNode consists of a dictionary which stores the the relationship of parent-child in a key-value pair manner. Each TrieNode, also consists of a flag indicating, whether it is end of the word.

\subsubsection{TokenTrie}
The \textit{TokenTrie} class stores sequences of tokenized words to enforce constraints during decoding. It consists of the following key methods:
\begin{itemize}
    \item \textbf{insert(token\_sequence)}: Inserts a sequence of tokens into the trie and flags the last token.
    \item \textbf{get\_next\_tokens(node)}: Retrieves the next possible tokens from the given node in the trie.
    \item \textbf{is\_complete\_word(token\_sequence)}: Checks if sequence of tokens forms a complete word.
\end{itemize}

\subsubsection{ConstrainedTextGenerator}
The class \textit{ConstrainedTextGenerator} builds a Trie, and integrates it with language model to perform constrained output generation, giving controlled yet flexible results.
\begin{itemize}
    \item \textbf{\_build\_trie\_from\_word\_list()} takes the word\_list and converts it into a Trie datastructure.
    \item \textbf{\_\_call\_\_()} implements the word-constrained decoding using trie, masking and greedy decoding technique. 
    \begin{enumerate}
        \item Initializes the TokenTrie and sets the current node to the root.
        \item Iteratively generates tokens up to max\_output\_len:
        \begin{itemize}
            \item Concatenates previously generated tokens with input tokens.
            \item Passes the sequence through the language model to obtain token logits.
            \item Extracts allowed next tokens from the Trie.
            \item Applies masking to restrict the output space.
            \item Selects the most probable token (greedy decoding strategy).
            \item Updates the Trie state to track word formation.
        \end{itemize}
        \item Terminates generation upon reaching the end-of-sequence token (eos\_token\_id) or exceeding max\_output\_len.
    \end{enumerate}
\end{itemize}

\subsection{Results and Conclusion}
The results mentioned in Table-1, demonstrate the effectiveness of Word-Constrained Decoding in improving output alignment with the reference text by enforcing a strict vocabulary constraint. The method produces better results as compared to the standard greedy decoding technique employed in previous task.


\newpage
\begin{table}[h!]
\begin{center}
\begin{tabular}{|l||c c c c||} 
\hline
Task & BLEU & ROUGE-1 & ROUGE-2 & ROUGE-LCS \\ [0.5ex] 
\hline\hline
\textbf{LLM Decoding Techniques} & & & & \\ \hline
Greedy Decoding  & \textbf{0.31} & \textbf{0.35} & \textbf{0.13} & \textbf{0.27} \\ 
\hdashline
Random Sampling with Temperature Scaling & & & &\\ 
$\tau$ = 0.5 & \textbf{0.29} & \textbf{0.30} & \textbf{0.11} & \textbf{0.24} \\ 
$\tau$ = 0.6 & 0.28 & 0.29 & 0.10 & 0.23 \\ 
$\tau$ = 0.7 & 0.26 & 0.26 & 0.09 & 0.20 \\ 
$\tau$ = 0.8 & 0.21 & 0.19 & 0.06 & 0.15 \\ 
$\tau$ = 0.9 & 0.20 & 0.18 & 0.05 & 0.15 \\ 
\hdashline 
Top-k sampling & & & &\\ 
k = 5 & 0.24 & 0.23 & 0.06 & 0.17 \\ 
k = 6 & \textbf{0.24} & \textbf{0.24} & \textbf{0.07} & \textbf{0.18} \\ 
k = 7 & 0.23 & 0.21 & 0.07 & 0.15 \\ 
k = 8 & 0.22 & 0.19 & 0.05 & 0.15 \\ 
k = 9 & 0.23 & 0.20 & 0.05 & 0.16 \\ 
k = 10 & 0.22 & 0.22 & 0.05 & 0.17 \\ 
\hdashline
Nucleus Sampling Decoding Techniques & & & &\\ 
p = 0.5 & \textbf{0.28} & \textbf{0.31} & \textbf{0.10} & \textbf{0.25} \\ 
p = 0.6 & 0.26 & 0.25 & 0.10 & 0.22 \\ 
p = 0.7 & 0.25 & 0.26 & 0.09 & 0.20 \\ 
p = 0.8 & 0.22 & 0.23 & 0.08 & 0.18 \\ 
p = 0.9 & 0.19 & 0.19 & 0.05 & 0.15 \\ 
\hline \hline
\textbf{Word Constrained Decoding} & \textbf{0.55} & \textbf{0.59} & \textbf{0.33} & \textbf{0.49} \\ \hline
\end{tabular}
\end{center}
\caption{Performance Metrics for different decoding techniques for Llama-2-7b-hf}
\end{table}

\begin{table}[h!]
\begin{center}
\begin{tabular}{|l||c c c c||} 
\hline
Decoding Technique & BLEU & ROUGE-1 & ROUGE-2 & ROUGE-LCS \\ [0.5ex] 
\hline\hline
Greedy Decoding  & \textbf{0.41} & \textbf{0.48} & \textbf{0.26} &\textbf{0.41} \\ 
\hline 
Random Sampling with Temperature Scaling & & & &\\ 
$\tau$ = 0.5 & \textbf{0.43} & 0.44 & 0.24 & 0.38 \\ 
$\tau$ = 0.6 & 0.41 & \textbf{0.46} & \textbf{0.24} & \textbf{0.39} \\ 
$\tau$ = 0.7 & 0.42 & 0.44 & 0.23 & 0.38 \\ 
$\tau$ = 0.8 & 0.36 & 0.39 & 0.18 & 0.33 \\ 
$\tau$ = 0.9 & 0.38 & 0.39 & 0.19 & 0.34 \\ 
\hline 
Top-k sampling & & & &\\ 
k = 5 & \textbf{0.42} & 0.41 & 0.20 & 0.36 \\ 
k = 6 & 0.41 & \textbf{0.44} & \textbf{0.22} & \textbf{0.37} \\ 
k = 7 & 0.40 & 0.43 & 0.18 & 0.35 \\ 
k = 8 & 0.39 & 0.40 & 0.18 & 0.33 \\ 
k = 9 & 0.35 & 0.35 & 0.14 & 0.29 \\ 
k = 10 & 0.33 & 0.38 & 0.17 & 0.33 \\ 
\hline 
Nucleus Sampling Decoding Techniques & & & &\\ 
p = 0.5 & 0.38 & 0.43 & 0.23 & 0.37 \\ 
p = 0.6 & 0.40 & 0.43 & 0.23 & 0.37 \\ 
p = 0.7 & \textbf{0.42} & \textbf{0.44} & \textbf{0.25} & \textbf{0.39} \\ 
p = 0.8 & 0.38 & 0.43 & 0.22 & 0.38 \\ 
p = 0.9 & 0.40 & 0.42 & 0.21 & 0.37 \\ 
\hline 
\end{tabular}
\end{center}
\caption{Performance Metrics for different decoding techniques for Llama-3.1-8B}
\end{table}

\section{Task 2: Staring into Medusa's Heads}

\subsection{Introduction to Parallel Decoding}
Traditional autoregressive language models generate text sequentially, predicting each token conditioned on all previously generated tokens. While effective, this approach suffers from high latency since each token requires a full forward pass through the model.

Medusa introduces a novel 	extbf{speculative parallel decoding} paradigm that enables the model to predict multiple future tokens simultaneously through specialized parallel prediction heads. This approach significantly accelerates text generation while maintaining reasonable accuracy, making it particularly useful for applications requiring low-latency responses, such as real-time translation and conversational AI.

\subsection{Medusa Architecture}
The Medusa framework enhances standard transformer models by integrating two crucial components:

\begin{itemize}
    \item \textbf{Multiple Prediction Heads}: Additional parallel output heads ($H_1, H_2, ..., H_K$) that predict future tokens simultaneously.
    \item \textbf{Verification Module}: A mechanism to validate the predicted token sequences using the base model’s probabilities.
\end{itemize}

For an input sequence $x_1, x_2, ..., x_t$, the base model computes the hidden states $h_t$. Each Medusa head $H_k$ then predicts the probability distribution of future tokens:

\begin{equation}
    P^{(k)}(x_{t+1}, ..., x_{t+k} \mid x_{1:t}) = \prod_{i=1}^{k} \text{softmax}(W_i^{(k)} h_t)
\end{equation}

where $W_i^{(k)}$ are learned projection matrices for position $i$ in head $k$. These projections allow each head to independently predict a fixed number of tokens ahead.

\subsection{Medusa Decoding Algorithm}
The complete Medusa decoding process follows an iterative approach, leveraging multiple prediction heads to enhance generation speed while ensuring accuracy via verification. The procedure is as follows:

\begin{algorithm}
\caption{Medusa Parallel Decoding Algorithm}
\begin{algorithmic}[1]
\REQUIRE Input prompt $x_{1:n}$, maximum length $L$, beam width $B$, number of Medusa heads $K$
\STATE Initialize candidate beam set $C \leftarrow \{ x_{1:n} \}$
\WHILE{$|x| < L$ \AND no EOS token in all candidates}
    \FOR{each candidate sequence $c \in C$}
        \STATE Compute base model hidden states $h_t$ for $c$
        \STATE Use $K$ Medusa heads to generate $M$ candidate continuations $S = \{s_1, ..., s_M\}$
        \STATE Compute scores for each candidate:
        \begin{equation}
        s(m) = \sum_{i=1}^{k} \log P_{\text{medusa}}^{(i)}(x_{t+i})
        \end{equation}
    \ENDFOR
    \STATE Select top-$B$ candidates by score
    \STATE Verify candidates using base model probabilities:
    \FOR{each candidate $s_m \in S$}
        \IF{$P_{\text{base}}(x_{t}) \geq \alpha \cdot P_{\text{medusa}}(x_{t})$ for all tokens}
            \STATE Accept $s_m$ into updated beam $C$
        \ELSE
            \STATE Discard $s_m$
        \ENDIF
    \ENDFOR
\ENDWHILE
\RETURN Best sequence from beam $C$
\end{algorithmic}
\end{algorithm}

\subsection{Key Components}

\subsubsection{Speculative Parallel Generation}
Each Medusa head generates multiple token candidates simultaneously, significantly reducing generation time. For example, with $K=4$ heads, we can predict 4 future tokens in parallel instead of processing them sequentially.

\subsubsection{Tree-Based Verification Mechanism}
Candidate sequences are structured into a prefix tree, where each node represents a token position. The verification phase follows a two-step process:
\begin{itemize}
    \item Compute base model probabilities for each candidate token sequence.
    \item Accept tokens satisfying the condition:
    \begin{equation}
        P_{\text{base}}(x_t) \geq \alpha \cdot P_{\text{medusa}}(x_t)
    \end{equation}
    where $\alpha$ is a predefined acceptance threshold.
\end{itemize}
Tokens failing the acceptance criteria are discarded, ensuring high-quality outputs.

\subsubsection{Beam Search Integration}
The algorithm maintains multiple promising candidates simultaneously, refining selections through:
\begin{equation}
    \text{Score}(c) = \sum_{i=1}^{t} \log P(x_i \mid x_{<i}) + \gamma \sum_{j=1}^{k} \log P_{\text{medusa}}(x_{t+j})
\end{equation}
where $\gamma$ controls the weight given to Medusa predictions, balancing efficiency and accuracy.

\subsection{Implementation Details}
Our implementation optimizes performance through:
\begin{itemize}
    \item \textbf{Adaptive Head Selection}: Dynamically enables Medusa heads based on prediction confidence.
    \item \textbf{Memory-Efficient Parameter Sharing}: Medusa heads share base model parameters via linear adaptations.
    \item \textbf{Cache Optimization}: Hidden states are reused across prediction heads to minimize redundant computations.
\end{itemize}

\subsection{Results Analysis}
Table 3 presents an evaluation of Medusa decoding across different configurations:

\begin{itemize}
    \item \textbf{Speed-Accuracy Tradeoff}: Medusa achieves a 3-5x speedup (RTF 0.03 vs 0.07) at the cost of a BLEU score reduction (0.12 vs 0.29).
    \item \textbf{Beam Width Effect}: Larger beam widths improve accuracy slightly but increase computation cost.
    \item \textbf{Optimal Head Count}: Increasing the number of Medusa heads improves parallelism but also raises the risk of cumulative errors.
\end{itemize}

The optimal configuration balances speed and accuracy, with beam width 2 and 2 Medusa heads achieving an RTF of 0.03 while retaining 41\% of the base model’s BLEU score.

\subsection{Conclusion}
Medusa decoding presents a promising technique for accelerating large language model inference by leveraging parallel token prediction. Despite minor accuracy trade-offs, its modular design allows for further refinements in:
\begin{itemize}
    \item Improved head architectures to reduce prediction errors.
    \item More sophisticated verification mechanisms to enhance token acceptance.
    \item Adaptive parameter tuning for better speed-accuracy balance.
\end{itemize}
This technique is particularly beneficial for latency-sensitive applications, such as machine translation and dialogue systems, where fast response times outweigh minor reductions in output fidelity.


\begin{table}[h!]
\begin{center}
\begin{tabular}{|l||c c c c c||} 
\hline
Decoding Technique & BLEU & ROUGE-1 & ROUGE-2 & ROUGE-LCS & RTF\\ [0.5ex] 
\hline\hline
\textbf{Single Medusa Head}  & 0.29 & 0.40 & 0.15 & 0.32 & 0.07 \\ 
\hline 
\textbf{Multiple Medusa Heads} & & & & & \\ 
Beam-Width: 2, Medusa-Heads: 2 & 0.12 & 0.30 & 0.09 & 0.22 & 0.03 \\ 
Beam-Width: 2, Medusa-Heads: 5 & 0.12 & 0.30 & 0.09 & 0.22 & 0.03 \\ 
Beam-Width: 5, Medusa-Heads: 2 & 0.11 & 0.29 & 0.10 & 0.23 & 0.08 \\ 
Beam-Width: 5, Medusa-Heads: 5 & 0.11 & 0.29 & 0.10 & 0.23 & 0.07 \\ 
Beam-Width: 10, Medusa-Heads: 2 & 0.11 & 0.28 & 0.10 & 0.22 & 0.16 \\ 
Beam-Width: 10, Medusa-Heads: 5 & 0.11 & 0.28 & 0.10 & 0.22 & 0.17 \\ 
\hline 
\end{tabular}
\end{center}
\caption{Performance Metrics for Medusa}
\end{table}

\newpageZ
\section{AI Contribution}
\label{sec:ai_contribution}

In the development of this project, \textbf{ChatGPT} \& \textbf{Deepseek} was utilized for occasional debugging and assistance in the implementation of various functions. The LLMs contributed by providing initial drafts for these functions, which were then reviewed, debugged, and refined to align with the problem requirements.

\section{Participant Contributions}
\label{sec:participant_contributions}

The project involved multiple stages of development, debugging, and report preparation. The following contributions were made by the participants:

\subsection{Aryan's Contributions}
\begin{itemize}
    \item Made the Report for the Assignment.
    \item Debugging and refining the implementations of :
    \begin{itemize}
        \item Random Sampling with Temperature Scaling of Task 0 : Introduction to LLM Decoding Techniques.
        \item Top-k Sampling of Task 0 : Introduction to LLM Decoding Techniques.
    \end{itemize}
\end{itemize}

\subsection{Anupam's Contributions}
\begin{itemize}
    \item Implemented and debugged the code for :
    \begin{itemize}
    \item Task 0 : Introduction to LLM Decoding Techniques
    \item Task 1 : Word Constraint Decoding
    \item Multiple Head Decoding for Task 2 : Staring into Medusa's Heads.
    \end{itemize}
    \item Made the Report for the project.
\end{itemize}

\subsection{Satush's Contributions}
\begin{itemize}
    \item Debugging and refining the implementations of:
    \begin{itemize}
    \item Implemented and debugged the code for initial part of multiple head decoding for Task 2 : Staring into Medusa's Heads.
    \item Initial part of multiple head decoding for Task 2 : Staring into Medusa's Heads
    \end{itemize}
\end{itemize}

The combination of AI-assisted function generation and participant debugging ensured that the project met its intended objectives.
\section{References}

\begin{itemize}
    \item \href{https://sites.google.com/view/medusa-llm}{Medusa: Parallel Decoding for LLMs}
    \item \href{https://huggingface.co/blog/constrained-beam-search}{Attention is All You Need}
\end{itemize}

\end{document}